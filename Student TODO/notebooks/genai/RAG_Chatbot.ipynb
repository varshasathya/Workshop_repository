{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "108d7aa1",
   "metadata": {},
   "source": [
    "\n",
    "# Session 3 Handsâ€‘On â€” Build a Mini RAG Chatbot (Local Flanâ€‘T5, No Streamlit)\n",
    "\n",
    "In this session, we will learn how to **retrieve and generate answers** using a local language model (Flanâ€‘T5 Small).  \n",
    "Weâ€™ll use the sample **college guide text** as our dataset and build a simple RAG (Retrievalâ€‘Augmented Generation) chatbot that works directly inside this notebook.\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this section, you will be able to:\n",
    "- Load and reuse precomputed **embeddings and FAISS index**\n",
    "- Understand how **retrieval** works in a RAG pipeline\n",
    "- Use a **local model (Flanâ€‘T5)** for text generation\n",
    "- Combine both into a **chatâ€‘like notebook interface**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85b60b0",
   "metadata": {},
   "source": [
    "\n",
    "## ðŸ”¹ What Is RAG?\n",
    "\n",
    "**Retrievalâ€‘Augmented Generation (RAG)** combines two parts:\n",
    "1. **Retriever** â€” finds the most relevant passages (chunks) for a given query.\n",
    "2. **Generator** â€” uses these retrieved texts to form a coherent answer.\n",
    "\n",
    "Think of it as an **openâ€‘book exam**:\n",
    "- The retriever is the student flipping through notes ðŸ“š.\n",
    "- The generator is the student writing the answer âœï¸.\n",
    "\n",
    "This makes AI more accurate and less â€œhallucinative.â€\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b21266",
   "metadata": {},
   "source": [
    "\n",
    "## 0) Setup (Run once)\n",
    "This installs all dependencies for embeddings, retrieval, generation, and the Streamlit chat UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0448ae5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set TRANSFORMERS_NO_TF and USE_TF\n"
     ]
    }
   ],
   "source": [
    "# Force Transformers to use PyTorch backend only\n",
    "import os\n",
    "os.environ['TRANSFORMERS_NO_TF'] = '1'\n",
    "os.environ['USE_TF'] = '0'\n",
    "print(\"Set TRANSFORMERS_NO_TF and USE_TF\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7584795",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.2\n",
      "[notice] To update, run: C:\\Users\\varsh\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.2\n",
      "[notice] To update, run: C:\\Users\\varsh\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Install runtime deps (CPU-friendly)\n",
    "!pip -q install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n",
    "!pip -q install sentence-transformers faiss-cpu transformers accelerate streamlit pdfminer.six\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e93b0bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\varsh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports ready\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os, json, textwrap, time, pathlib\n",
    "import numpy as np\n",
    "from typing import List, Dict\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from openai import OpenAI\n",
    "\n",
    "print(\"Imports ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1a59ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c7111bb0",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Chunk the document\n",
    "RAG works best when documents are split into small, overlapping chunks.\n",
    "We'll use a simple fixedâ€‘size chunker (you can replace with smarter chunkers later).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d9ae01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2,\n",
       " {'id': 0,\n",
       "  'text': 'SRM College Guide (Sample) Admissions: - Application opens in January; last date is March 31. - Entrance exams are conducted in April; results by May 15. Library: - Open Monâ€“Sat, 8:00 AM â€“ 8:00 PM; Sun, 10:00 AM â€“ 4:00 PM. - Late fees: â‚¹2 per day for overdue books. - Digital library: Use your college email to access eâ€‘journals. Attendance & Exams: - Minimum attendance: 75% per course. - Internal assessments: 2 tests + 1 assignment per semester. - Endâ€‘semester exams: 60% of total grade. - Makeâ€‘up exams allowed only with medical certificate. Labs & Wiâ€‘Fi: - Labs open 9:00 AM â€“ 6:00 PM on weekdays. - Wiâ€‘Fi: Connect to \"SRMâ€‘Campus\"; login with student ID. Clubs & Events: -'})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def chunk_text(text: str, chunk_size: int = 400, overlap: int = 60) -> List[Dict]:\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    idx = 0\n",
    "    while start < len(words):\n",
    "        end = min(len(words), start + chunk_size)\n",
    "        chunk_words = words[start:end]\n",
    "        chunk = \" \".join(chunk_words)\n",
    "        chunks.append({\"id\": idx, \"text\": chunk})\n",
    "        if end == len(words):\n",
    "            break\n",
    "        start = end - overlap\n",
    "        idx += 1\n",
    "    return chunks\n",
    "\n",
    "with open(\"data/college_guide.txt\",\"r\",encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "chunks = #TODO: Chunk the text by passing appropriate arguments\n",
    "#Keep chunk_size - 120 and overlap - 20\n",
    "len(chunks), chunks[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f830c0f",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Build Embeddings + FAISS Index\n",
    "We'll use `all-MiniLM-L6-v2` (fast and accurate).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d806aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed 2 chunks with dim=384\n"
     ]
    }
   ],
   "source": [
    "\n",
    "EMBED_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "embedder = SentenceTransformer(\"\")#TODO: Load the embedding model\n",
    "\n",
    "texts = [c[\"text\"] for c in chunks]\n",
    "emb = embedder.encode(texts, convert_to_numpy=True, normalize_embeddings=True)\n",
    "\n",
    "dim = emb.shape[1]\n",
    "index = faiss.IndexFlatIP(dim)  # cosine sim if vectors normalized\n",
    "index.add(emb)\n",
    "\n",
    "np.save(\"data/embeddings.npy\", emb)\n",
    "import json\n",
    "with open(\"data/chunks.json\",\"w\",encoding=\"utf-8\") as f:\n",
    "    json.dump(chunks, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Indexed {len(chunks)} chunks with dim={dim}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a0609f",
   "metadata": {},
   "source": [
    "## 4) Retrieval function\n",
    "Retrieval finds the most relevant text chunks for a given question using **cosine similarity** between embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f43490",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.20035353302955627,\n",
       "  'id': 0,\n",
       "  'text': 'SRM College Guide (Sample) Admissions: - Application opens in January; last date is March 31. - Entrance exams are conducted in April; results by May 15. Library: - Open Monâ€“Sat, 8:00 AM â€“ 8:00 PM; Sun, 10:00 AM â€“ 4:00 PM. - Late fees: â‚¹2 per day for overdue books. - Digital library: Use your college email to access eâ€‘journals. Attendance & Exams: - Minimum attendance: 75% per course. - Internal assessments: 2 tests + 1 assignment per semester. - Endâ€‘semester exams: 60% of total grade. - Makeâ€‘up exams allowed only with medical certificate. Labs & Wiâ€‘Fi: - Labs open 9:00 AM â€“ 6:00 PM on weekdays. - Wiâ€‘Fi: Connect to \"SRMâ€‘Campus\"; login with student ID. Clubs & Events: -'},\n",
       " {'score': 0.15765821933746338,\n",
       "  'id': 1,\n",
       "  'text': '9:00 AM â€“ 6:00 PM on weekdays. - Wiâ€‘Fi: Connect to \"SRMâ€‘Campus\"; login with student ID. Clubs & Events: - Tech clubs: AI/ML Club (Fri 5 PM), Robotics Club (Wed 4 PM). - Cultural: Music Club (Tue 5 PM), Drama Society (Thu 5 PM). - Annual \"Innovate Fest\" in September; project submissions due Aug 20. Hostel & Mess: - Hostel entry closes at 9:30 PM. - Mess provides veg & nonâ€‘veg; weekly menu on the portal every Sunday. Contacts: - Admissions: admissions@srm.edu, +91â€‘44â€‘1234â€‘5678 - Library Helpdesk: library@srm.edu - IT Support: itsupport@srm.edu'},\n",
       " {'score': -3.4028234663852886e+38,\n",
       "  'id': 1,\n",
       "  'text': '9:00 AM â€“ 6:00 PM on weekdays. - Wiâ€‘Fi: Connect to \"SRMâ€‘Campus\"; login with student ID. Clubs & Events: - Tech clubs: AI/ML Club (Fri 5 PM), Robotics Club (Wed 4 PM). - Cultural: Music Club (Tue 5 PM), Drama Society (Thu 5 PM). - Annual \"Innovate Fest\" in September; project submissions due Aug 20. Hostel & Mess: - Hostel entry closes at 9:30 PM. - Mess provides veg & nonâ€‘veg; weekly menu on the portal every Sunday. Contacts: - Admissions: admissions@srm.edu, +91â€‘44â€‘1234â€‘5678 - Library Helpdesk: library@srm.edu - IT Support: itsupport@srm.edu'},\n",
       " {'score': -3.4028234663852886e+38,\n",
       "  'id': 1,\n",
       "  'text': '9:00 AM â€“ 6:00 PM on weekdays. - Wiâ€‘Fi: Connect to \"SRMâ€‘Campus\"; login with student ID. Clubs & Events: - Tech clubs: AI/ML Club (Fri 5 PM), Robotics Club (Wed 4 PM). - Cultural: Music Club (Tue 5 PM), Drama Society (Thu 5 PM). - Annual \"Innovate Fest\" in September; project submissions due Aug 20. Hostel & Mess: - Hostel entry closes at 9:30 PM. - Mess provides veg & nonâ€‘veg; weekly menu on the portal every Sunday. Contacts: - Admissions: admissions@srm.edu, +91â€‘44â€‘1234â€‘5678 - Library Helpdesk: library@srm.edu - IT Support: itsupport@srm.edu'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def retrieve(query: str, k: int = 4):\n",
    "    q = embedder.encode([query], convert_to_numpy=True, normalize_embeddings=True)\n",
    "    scores, idxs = index.search(q, k)\n",
    "    results = []\n",
    "    for score, idx_ in zip(scores[0], idxs[0]):\n",
    "        payload = chunks[int(idx_)]\n",
    "        results.append({\"score\": float(score), \"id\": payload[\"id\"], \"text\": payload[\"text\"]})\n",
    "    return results\n",
    "\n",
    "retrieve(\"What are library timings?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1c873f",
   "metadata": {},
   "source": [
    "\n",
    "## 5) Load the Local Textâ€‘Generation Model\n",
    "\n",
    "Weâ€™ll use **Flanâ€‘T5 Small**, a lightweight open model that runs offline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9619f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "MODEL = \"google/flan-t5-small\"\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained()# TODO: Load tokenizer\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained()# TODO: Load model\n",
    "\n",
    "gen = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=256)\n",
    "\n",
    "print(\"Local model loaded successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc760c1",
   "metadata": {},
   "source": [
    "\n",
    "## 6)  Compose the RAG Prompt\n",
    "\n",
    "The **prompt** combines:\n",
    "- The userâ€™s **question**\n",
    "- The **retrieved context** from the FAISS index\n",
    "\n",
    "This helps the model focus only on factual data from our dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c425d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt template ready\n"
     ]
    }
   ],
   "source": [
    "#TODO: Define a prompt template and provide appropriate instructions for the model to work\n",
    "PROMPT_TEMPLATE = \"\"\"Your intruction foes here.......\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\"\"\"\n",
    "print(\"Prompt template ready\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d8d5a9",
   "metadata": {},
   "source": [
    "\n",
    "## Combine Everything: The RAG Answer Function\n",
    "\n",
    "This function retrieves context, builds a prompt, and generates an answer using Flanâ€‘T5.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b743c36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'75% per course.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def rag_answer(query, k=1):\n",
    "    # Retrieve topâ€‘k chunks (list of dicts: {'score','id','text'})\n",
    "    retrieved = retrieve(query, k)\n",
    "    ctx = \"\\n\\n\".join([item[\"text\"] for item in retrieved])\n",
    "\n",
    "    # Build prompt\n",
    "    prompt = PROMPT_TEMPLATE.format(question=query, context=ctx)\n",
    "\n",
    "    # Generate response\n",
    "    raw = gen(prompt)[0][\"generated_text\"].strip()\n",
    "    one_line = raw.splitlines()[0]\n",
    "\n",
    "    return one_line, retrieved\n",
    "\n",
    "# Quick test\n",
    "rag_answer(\"What is the minimum attendance requirement?\")[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499f58ba",
   "metadata": {},
   "source": [
    "\n",
    "## Step 6 â€” Interactive Chat Session\n",
    "\n",
    "Now letâ€™s turn it into a simple chat loop inside the notebook!\n",
    "Type questions and see how your model responds using the college dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "024dc3df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mini RAG Chat (type 'exit' to quit)\n",
      "\n",
      "\n",
      "ðŸ¤– Answer: 75% per course.\n",
      "\n",
      "ðŸ“š Contexts used:\n",
      "  â€¢ (score 0.352) SRM College Guide (Sample) Admissions: - Application opens in January; last date is March ...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ðŸ¤– Answer: January; last date is March 31. - Entrance exams are conducted in April; results by May 15. Library: - Open Monâ€“Sat, 8:00 AM â€“ 8:00 PM; Sun, 10:00 AM â€“ 4:00 PM. - Late fees: 2 per day for overdue books. - Digital library: Use your college email to access ejournals. Attendance & Exams: - Minimum attendance: 75% per course. - Internal assessments: 2 tests + 1 assignment per semester. - Endsemester exams: 60% of total grade. - Makeup exams allowed only with medical certificate. Labs & WiFi: - Labs open 9:00 AM â€“ 6:00 PM on weekdays. - WiFi: Connect to \"SRMCampus\"; login with student ID. Clubs & Events:\n",
      "\n",
      "ðŸ“š Contexts used:\n",
      "  â€¢ (score 0.339) SRM College Guide (Sample) Admissions: - Application opens in January; last date is March ...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ðŸ¤– Answer: January; last date is March 31.\n",
      "\n",
      "ðŸ“š Contexts used:\n",
      "  â€¢ (score 0.299) SRM College Guide (Sample) Admissions: - Application opens in January; last date is March ...\n",
      "--------------------------------------------------------------------------------\n",
      "ðŸ‘‹ Exiting chat.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Mini RAG Chat (type 'exit' to quit)\\n\")\n",
    "\n",
    "while True:\n",
    "    q = input(\"Ask a question: \")\n",
    "    if q.lower() in [\"exit\", \"quit\"]:\n",
    "        print(\"ðŸ‘‹ Exiting chat.\")\n",
    "        break\n",
    "\n",
    "    answer, ctx = rag_answer(q)\n",
    "\n",
    "    print(\"\\nðŸ¤– Answer:\", answer)\n",
    "    print(\"\\nðŸ“š Contexts used:\")\n",
    "    for item in ctx:  # list of dicts {'score','id','text'}\n",
    "        print(f\"  â€¢ (score {item['score']:.3f}) {item['text'][:90]}...\")\n",
    "    print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62a0cd7",
   "metadata": {},
   "source": [
    "## Mini Challenges\n",
    "\n",
    "1) **Add content:** Open `data/college_guide.txt` and add 3â€“5 new bullet points (e.g., canteen timings, sports complex rules). Reâ€‘run *steps 2â€“3* to rebuild chunks and index.  \n",
    "2) **Tune retrieval:** Change `Topâ€‘K`, try 2, 4, 6. When do answers improve?  \n",
    "3) **Guardrails:** In `PROMPT_TEMPLATE`, add a rule: *â€œIf you are not sure, ask a followâ€‘up question.â€* Does behavior change?  \n",
    "4) **Model swap:** Replace `flanâ€‘t5-small` with openai or other powerfull models using API.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e15bf84",
   "metadata": {},
   "source": [
    "\n",
    "## Recall\n",
    "\n",
    "You have now built a fully functional **Retrievalâ€‘Augmented Generation (RAG)** chatbot using your own dataset.\n",
    "\n",
    "**Pipeline recap:**\n",
    "```\n",
    "Dataset (college_guide.txt)\n",
    "     â†“\n",
    "Chunks â†’ Embeddings â†’ FAISS Index\n",
    "     â†“\n",
    "Retriever â†’ Prompt Composer â†’ Generator\n",
    "     â†“\n",
    "Interactive Q&A Assistant\n",
    "```\n",
    "\n",
    "**Key takeaways:**\n",
    "- RAG helps keep LLMs factual by grounding answers in real data.\n",
    "- Retrieval quality = context quality â†’ better answers.\n",
    "- This same method powers modern chatbots and assistants today.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
