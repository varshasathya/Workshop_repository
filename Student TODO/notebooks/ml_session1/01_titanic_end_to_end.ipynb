{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Session 1: End-to-End ML Hands-on (Titanic)\n",
        "\n",
        "In this hands-on, you'll build an end-to-end ML workflow on the Titanic dataset using pandas and scikit-learn.\n",
        "\n",
        "Learning goals:\n",
        "- Understand the full flow: problem framing → EDA → preprocessing → modeling → evaluation → iteration → export → inference.\n",
        "- Learn how `Pipeline` and `ColumnTransformer` organize preprocessing and models.\n",
        "- Practice evaluating models beyond accuracy and prepare a Kaggle-style submission.\n",
        "- Mini-challenge: feature engineering at the end to improve accuracy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install minimal dependencies (Colab-friendly). \n",
        "!pip -q install pandas scikit-learn seaborn matplotlib joblib requests\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Imports and configuration\n",
        "\n",
        "Technical notes:\n",
        "- pandas: data loading and manipulation.\n",
        "- seaborn/matplotlib: quick EDA visualizations.\n",
        "- scikit-learn: split, preprocessing, pipelines, models, metrics, hyperparameter search.\n",
        "- joblib: save/load trained pipelines.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import requests\n",
        "\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,\n",
        "                             confusion_matrix, ConfusionMatrixDisplay, RocCurveDisplay)\n",
        "import joblib\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data access (dual-mode)\n",
        "\n",
        "We'll support two approaches:\n",
        "- Download from public URLs into `/content/data/raw/` (best for Colab and reproducibility).\n",
        "- Load from local repo path `../data/raw/` (if running locally).\n",
        "\n",
        "Set `USE_URLS = True` to download; otherwise, the notebook will try local paths.\n",
        "\n",
        "TODO: Update `TRAIN_URL` and `TEST_URL` to your hosted raw CSV links if using URLs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### The data\n",
        "\n",
        "To view the competition data, open the Titanic competition page and click the “Data” tab. You’ll see a list of files:\n",
        "\n",
        "1) train.csv  \n",
        "- Contains information for 891 passengers (one row per passenger).  \n",
        "- The target column is `Survived`:\n",
        "  - `1` = the passenger survived\n",
        "  - `0` = the passenger did not survive  \n",
        "For example, the first row (Mr. Owen Harris Braund) has `Survived = 0` and `Age = 22`.\n",
        "\n",
        "2) test.csv  \n",
        "- Contains 418 passengers for which we must predict `Survived`.  \n",
        "- It does not include a `Survived` column; those labels are hidden for scoring.  \n",
        "- We will train on `train.csv`, then generate predictions for `test.csv` to create a Kaggle submission.\n",
        "\n",
        "We’ll use these two files throughout our end-to-end ML workflow: EDA → preprocessing → modeling → evaluation → export → inference → submission."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "DATA_DIR_COLAB = '/content/data/raw'\n",
        "DATA_DIR_LOCAL = os.path.join('..', 'data', 'raw')\n",
        "train_path = os.path.join(DATA_DIR_LOCAL, 'train.csv')\n",
        "test_path = os.path.join(DATA_DIR_LOCAL, 'test.csv')\n",
        "\n",
        "#TODO: Read Train and Test data\n",
        "train_df = pd.read_csv()\n",
        "test_df = pd.read_csv()\n",
        "train_df.shape, test_df.shape\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Problem framing\n",
        "We are solving a binary classification problem: predict `Survived` (0/1) from passenger features.\n",
        "Common features include `Pclass`, `Sex`, `Age`, `SibSp`, `Parch`, `Fare`, and `Embarked`. The `Name`, `Ticket`, and `Cabin` fields are often noisy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "target_col = 'Survived'\n",
        "X = train_df.drop(columns=[target_col])\n",
        "y = train_df[target_col]\n",
        "X.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Quick EDA\n",
        "Goals:\n",
        "- Inspect schema, missingness, and distributions.\n",
        "- Identify candidate features for modeling and preprocessing needs.\n",
        "\n",
        "What to look for:\n",
        "- High missingness columns like `Cabin` (consider dropping) vs moderate ones like `Age` (impute).\n",
        "- Categorical cardinality for `Ticket`, `Cabin`, `Name` (often excluded in baselines).\n",
        "- Target leakage risks (none obvious here, but always check temporal or post-outcome fields).\n",
        "\n",
        "Exercise: Add one more insightful plot (e.g., `Survived` by `Sex` or `Pclass`).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "display(train_df.head())\n",
        "train_df.info()\n",
        "train_df.isna().mean().sort_values(ascending=False).to_frame('missing_frac')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(10,4))\n",
        "sns.countplot(data=train_df, x='Survived', ax=axes[0])\n",
        "axes[0].set_title('Target balance')\n",
        "\n",
        "#TODO: Plot Age distribution\n",
        "sns.histplot(data=\"\", x=\"\", kde=True, ax=axes[1])\n",
        "axes[1].set_title('Age distribution')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train/validation split\n",
        "We hold out a validation set to estimate generalization. We stratify by the target to preserve class balance.\n",
        "\n",
        "Key teaching points:\n",
        "- Split BEFORE any preprocessing to avoid leakage.\n",
        "- Fit imputers/encoders/scalers ONLY on training folds.\n",
        "- Keep `X_train`, `X_valid`, `y_train`, `y_valid` unchanged until preprocessing steps.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train, X_valid, y_train, y_valid = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "X_train.shape, X_valid.shape\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Preprocessing (classic, step-by-step without Pipelines)\n",
        "We will implement preprocessing explicitly so students can see each transformation:\n",
        "- **Select features**: numeric vs categorical.\n",
        "- **Fit imputers on train only** and transform train/valid.\n",
        "- **One-hot encode categoricals** fitted on train only; ignore unknowns at valid/test.\n",
        "- **Scale numeric features** fitted on train only.\n",
        "\n",
        "This mirrors what a `Pipeline` does internally, but shows each step so you can inspect intermediate outputs and understand data leakage risks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define feature groups\n",
        "numeric_features = ['Age', 'Fare', 'SibSp', 'Parch']\n",
        "categorical_features = ['Pclass', 'Sex', 'Embarked']\n",
        "\n",
        "# 1) Fit imputers on training data only\n",
        "num_imputer = SimpleImputer(strategy='median')\n",
        "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
        "\n",
        "X_train_num = num_imputer.fit_transform(X_train[numeric_features])\n",
        "X_valid_num = num_imputer.transform(X_valid[numeric_features])\n",
        "\n",
        "#TODO: Transform validation data\n",
        "X_train_cat_imputed = cat_imputer.fit_transform(X_train[categorical_features])\n",
        "X_valid_cat_imputed = \"\"#TODO: Transform validation data\n",
        "\n",
        "# 2) One-hot encode categoricals: fit on train only\n",
        "onehot = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
        "X_train_cat_oh = onehot.fit_transform(X_train_cat_imputed)\n",
        "X_valid_cat_oh = \"\"#TODO: Transform validation data\n",
        "\n",
        "# 3) Scale numeric features: fit on train only\n",
        "scaler = StandardScaler()\n",
        "X_train_num_scaled = scaler.fit_transform(X_train_num)\n",
        "X_valid_num_scaled = \"\"#TODO: Transform validation data\n",
        "\n",
        "# 4) Concatenate numeric and categorical arrays\n",
        "X_train_prepared = np.hstack([X_train_num_scaled, X_train_cat_oh])\n",
        "X_valid_prepared = np.hstack([X_valid_num_scaled, X_valid_cat_oh])\n",
        "\n",
        "X_train_prepared.shape, X_valid_prepared.shape\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Baseline model: Logistic Regression on prepared arrays\n",
        "We train directly on `X_train_prepared` and evaluate on `X_valid_prepared`.\n",
        "This keeps the modeling step separate from preprocessing for teaching clarity.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "log_reg = LogisticRegression(max_iter=1000)\n",
        "# model.fit(features,labels)\n",
        "#TODO: Fit the model\n",
        "\n",
        "y_pred = log_reg.predict(X_valid_prepared)\n",
        "y_proba = log_reg.predict_proba(X_valid_prepared)[:, 1]\n",
        "\n",
        "acc = accuracy_score(y_valid, y_pred)\n",
        "prec = precision_score(y_valid, y_pred)\n",
        "rec = recall_score(y_valid, y_pred)\n",
        "f1 = f1_score(y_valid, y_pred)\n",
        "roc = roc_auc_score(y_valid, y_proba)\n",
        "\n",
        "print('Baseline metrics:')\n",
        "print(f\"- Accuracy : {float(acc):.4f}\")\n",
        "print(f\"- Precision: {float(prec):.4f}\")\n",
        "print(f\"- Recall   : {float(rec):.4f}\")\n",
        "print(f\"- F1       : {float(f1):.4f}\")\n",
        "print(f\"- ROC AUC  : {float(roc):.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1, 2, figsize=(10,4))\n",
        "ConfusionMatrixDisplay.from_predictions(y_valid, y_pred, ax=ax[0])\n",
        "ax[0].set_title('Confusion Matrix')\n",
        "RocCurveDisplay.from_predictions(y_valid, y_proba, ax=ax[1])\n",
        "ax[1].set_title('ROC Curve')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Simple hyperparameter search with GridSearchCV\n",
        "We keep it simple: use `GridSearchCV` over a few `C` values on the preprocessed training arrays, then evaluate the best model on `X_valid_prepared`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = { 'C': [0.1, 1.0, 3.0] }\n",
        "base_lr = LogisticRegression(max_iter=1000)\n",
        "search = GridSearchCV(base_lr, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "search.fit(X_train_prepared, y_train)\n",
        "\n",
        "best_C = search.best_params_['C']\n",
        "best_acc_cv = search.best_score_\n",
        "best_C, best_acc_cv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train best estimator from GridSearchCV on the same prepared arrays\n",
        "best_model = search.best_estimator_\n",
        "best_model.fit(X_train_prepared, y_train)\n",
        "\n",
        "\n",
        "y_pred_best = \"\"#TODO: Predict using the best model\n",
        "y_proba_best = best_model.predict_proba(X_valid_prepared)[:, 1]\n",
        "acc_b = accuracy_score(y_valid, y_pred_best)\n",
        "prec_b = precision_score(y_valid, y_pred_best)\n",
        "rec_b = recall_score(y_valid, y_pred_best)\n",
        "f1_b = f1_score(y_valid, y_pred_best)\n",
        "roc_b = roc_auc_score(y_valid, y_proba_best)\n",
        "\n",
        "print('Tuned model metrics:')\n",
        "print(f\"- Accuracy : {float(acc_b):.4f}\")\n",
        "print(f\"- Precision: {float(prec_b):.4f}\")\n",
        "print(f\"- Recall   : {float(rec_b):.4f}\")\n",
        "print(f\"- F1       : {float(f1_b):.4f}\")\n",
        "print(f\"- ROC AUC  : {float(roc_b):.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Error analysis\n",
        "Inspect where the model fails to guide improvements.\n",
        "\n",
        "Suggestions to try:\n",
        "- Add `FamilySize = SibSp + Parch + 1` and retrain.\n",
        "- Extract `Title` from `Name` and group rare titles.\n",
        "- Try tree-based models (`RandomForestClassifier`) that may capture non-linearities without scaling.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "errors = X_valid.copy()\n",
        "errors['y_true'] = y_valid.values\n",
        "errors['y_pred'] = y_pred_best\n",
        "errors[errors['y_true'] != errors['y_pred']].head(10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Export the trained components\n",
        "We will export the fitted preprocessing objects and the `LogisticRegression` model separately with `joblib`:\n",
        "- `num_imputer.joblib`, `cat_imputer.joblib`\n",
        "- `onehot.joblib`, `scaler.joblib`\n",
        "- `log_reg.joblib`\n",
        "\n",
        "This allows loading them later to preprocess new data in the exact same way.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "joblib.dump(num_imputer, 'num_imputer.joblib')\n",
        "joblib.dump(cat_imputer, 'cat_imputer.joblib')\n",
        "joblib.dump(onehot, 'onehot.joblib')\n",
        "joblib.dump(scaler, 'scaler.joblib')\n",
        "joblib.dump(best_model, 'log_reg.joblib')\n",
        "[os.path.getsize(p) for p in ['num_imputer.joblib','cat_imputer.joblib','onehot.joblib','scaler.joblib','log_reg.joblib']]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Inference on test set and Kaggle-style submission\n",
        "For test-time, we must apply the SAME preprocessing fitted on the full training data:\n",
        "1) Fit imputers/encoder/scaler on full `X, y` (train only).\n",
        "2) Transform `test_df` with those fitted objects (never refit on test).\n",
        "3) Predict with the trained model.\n",
        "4) Build `submission.csv` with `PassengerId` and `Survived`.\n",
        "\n",
        "Notes:\n",
        "- Our `OneHotEncoder(handle_unknown='ignore')` safely handles unseen categories at test-time.\n",
        "- Always keep the exact same columns/order by reusing the same fitted encoder and scaler.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def fit_full_preprocessing_and_model(X, y, numeric_features, categorical_features, C):\n",
        "    num_imputer = SimpleImputer(strategy='median')\n",
        "    cat_imputer = SimpleImputer(strategy='most_frequent')\n",
        "    onehot = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
        "    scaler = StandardScaler()\n",
        "\n",
        "    X_num = num_imputer.fit_transform(X[numeric_features])\n",
        "    X_cat = cat_imputer.fit_transform(X[categorical_features])\n",
        "    X_cat_oh = onehot.fit_transform(X_cat)\n",
        "    X_num_s = scaler.fit_transform(X_num)\n",
        "    X_prep = np.hstack([X_num_s, X_cat_oh])\n",
        "\n",
        "    model = LogisticRegression(max_iter=1000, C=C)\n",
        "    model.fit(X_prep, y)\n",
        "    return num_imputer, cat_imputer, onehot, scaler, model\n",
        "\n",
        "\n",
        "def transform_with_fitted(num_imputer, cat_imputer, onehot, scaler, df, numeric_features, categorical_features):\n",
        "    D_num = num_imputer.transform(df[numeric_features])\n",
        "    D_cat = cat_imputer.transform(df[categorical_features])\n",
        "    D_cat_oh = onehot.transform(D_cat)\n",
        "    D_num_s = scaler.transform(D_num)\n",
        "    return np.hstack([D_num_s, D_cat_oh])\n",
        "\n",
        "\n",
        "# 1) Fit on full train, 2) Transform test, 3) Predict and save\n",
        "num_imp_f, cat_imp_f, onehot_f, scaler_f, final_model = fit_full_preprocessing_and_model(\n",
        "    X, y, numeric_features, categorical_features, C=best_C\n",
        ")\n",
        "T_prepared = transform_with_fitted(num_imp_f, cat_imp_f, onehot_f, scaler_f, test_df, numeric_features, categorical_features)\n",
        "\n",
        "test_pred = \"\"#TODO: Predict using the final model on prepared test data\n",
        "submission = pd.DataFrame({\n",
        "    'PassengerId': test_df['PassengerId'],\n",
        "    'Survived': test_pred\n",
        "})\n",
        "submission.to_csv('submission.csv', index=False)\n",
        "submission.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Mini-challenge: Add FamilySize and compare\n",
        "We will create a `FamilySize` feature (`SibSp + Parch + 1`), include it in the numeric features, retrain preprocessing + Logistic Regression, and compare metrics and a second submission file (`submission_family.csv`).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add FamilySize\n",
        "X_train_fs = X_train.copy(); X_valid_fs = X_valid.copy(); X_fs = X.copy(); test_df_fs = test_df.copy()\n",
        "for df_ in [X_train_fs, X_valid_fs, X_fs, test_df_fs]:\n",
        "    df_['FamilySize'] = df_['SibSp'] + df_['Parch'] + 1\n",
        "\n",
        "# Features\n",
        "numeric_features_fs = ['Age', 'Fare', 'SibSp', 'Parch', 'FamilySize']\n",
        "categorical_features_fs = ['Pclass', 'Sex', 'Embarked']\n",
        "\n",
        "# Preprocess (fit on train, transform valid)\n",
        "num_imp = SimpleImputer(strategy='median'); cat_imp = SimpleImputer(strategy='most_frequent')\n",
        "onehot = OneHotEncoder(handle_unknown='ignore', sparse_output=False); scaler = StandardScaler()\n",
        "Xtr_num = num_imp.fit_transform(X_train_fs[numeric_features_fs]); Xva_num = num_imp.transform(X_valid_fs[numeric_features_fs])\n",
        "Xtr_cat = cat_imp.fit_transform(X_train_fs[categorical_features_fs]); Xva_cat = cat_imp.transform(X_valid_fs[categorical_features_fs])\n",
        "Xtr = np.hstack([scaler.fit_transform(Xtr_num), onehot.fit_transform(Xtr_cat)])\n",
        "Xva = np.hstack([scaler.transform(Xva_num), onehot.transform(Xva_cat)])\n",
        "\n",
        "# Simple GridSearchCV\n",
        "search_fs = GridSearchCV(LogisticRegression(max_iter=1000), {'C':[0.1,1.0,3.0]}, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "search_fs.fit(Xtr, y_train)\n",
        "\n",
        "# Evaluate\n",
        "best_fs = search_fs.best_estimator_\n",
        "pred_fs = best_fs.predict(Xva)\n",
        "proba_fs = best_fs.predict_proba(Xva)[:,1]\n",
        "acc_fs = accuracy_score(y_valid, pred_fs); prec_fs = precision_score(y_valid, pred_fs)\n",
        "rec_fs = recall_score(y_valid, pred_fs); f1_fs = f1_score(y_valid, pred_fs); roc_fs = roc_auc_score(y_valid, proba_fs)\n",
        "\n",
        "print('FamilySize model metrics:')\n",
        "print(f\"- Accuracy : {float(acc_fs):.4f}\")\n",
        "print(f\"- Precision: {float(prec_fs):.4f}\")\n",
        "print(f\"- Recall   : {float(rec_fs):.4f}\")\n",
        "print(f\"- F1       : {float(f1_fs):.4f}\")\n",
        "print(f\"- ROC AUC  : {float(roc_fs):.4f}\")\n",
        "\n",
        "# Submission with full train\n",
        "num_imp_f, cat_imp_f, onehot_f, scaler_f, final_fs = fit_full_preprocessing_and_model(\n",
        "    X_fs, y, numeric_features_fs, categorical_features_fs, C=search_fs.best_params_['C']\n",
        ")\n",
        "T_fs = transform_with_fitted(num_imp_f, cat_imp_f, onehot_f, scaler_f, test_df_fs, numeric_features_fs, categorical_features_fs)\n",
        "sub_fs = pd.DataFrame({'PassengerId': test_df_fs['PassengerId'], 'Survived': final_fs.predict(T_fs)})\n",
        "sub_fs.to_csv('submission_family.csv', index=False)\n",
        "sub_fs.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
