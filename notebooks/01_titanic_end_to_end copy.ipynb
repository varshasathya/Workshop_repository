{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Session 1: End-to-End ML Hands-on (Titanic)\n",
        "\n",
        "In this hands-on, you'll build an end-to-end ML workflow on the Titanic dataset using pandas and scikit-learn.\n",
        "\n",
        "Learning goals:\n",
        "- Understand the full flow: problem framing → EDA → preprocessing → modeling → evaluation → iteration → export → inference.\n",
        "- Learn how `Pipeline` and `ColumnTransformer` organize preprocessing and models.\n",
        "- Practice evaluating models beyond accuracy and prepare a Kaggle-style submission.\n",
        "- Mini-challenge: feature engineering at the end to improve accuracy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install minimal dependencies (Colab-friendly). \n",
        "!pip -q install pandas scikit-learn seaborn matplotlib joblib requests\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Imports and configuration\n",
        "\n",
        "Technical notes:\n",
        "- pandas: data loading and manipulation.\n",
        "- seaborn/matplotlib: quick EDA visualizations.\n",
        "- scikit-learn: split, preprocessing, pipelines, models, metrics, hyperparameter search.\n",
        "- joblib: save/load trained pipelines.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'pandas'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[0;32m      4\u001b[0m warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import requests\n",
        "\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,\n",
        "                             confusion_matrix, ConfusionMatrixDisplay, RocCurveDisplay)\n",
        "import joblib\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data access (dual-mode)\n",
        "\n",
        "We'll support two approaches:\n",
        "- Download from public URLs into `/content/data/raw/` (best for Colab and reproducibility).\n",
        "- Load from local repo path `../data/raw/` (if running locally).\n",
        "\n",
        "Set `USE_URLS = True` to download; otherwise, the notebook will try local paths.\n",
        "\n",
        "TODO: Update `TRAIN_URL` and `TEST_URL` to your hosted raw CSV links if using URLs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Toggle between URL download and local path\n",
        "USE_URLS = True  # set to False to use local CSVs under ../data/raw/\n",
        "\n",
        "# TODO: replace with your public raw URLs (e.g., GitHub raw).\n",
        "TRAIN_URL = 'https://raw.githubusercontent.com/your-org/your-repo/main/data/raw/train.csv'\n",
        "TEST_URL  = 'https://raw.githubusercontent.com/your-org/your-repo/main/data/raw/test.csv'\n",
        "\n",
        "DATA_DIR_COLAB = '/content/data/raw'\n",
        "DATA_DIR_LOCAL = os.path.join('..', 'data', 'raw')\n",
        "os.makedirs(DATA_DIR_COLAB, exist_ok=True)\n",
        "\n",
        "def download_if_needed(url: str, dst_dir: str) -> str:\n",
        "    filename = os.path.basename(url)\n",
        "    dst_path = os.path.join(dst_dir, filename)\n",
        "    if not os.path.exists(dst_path):\n",
        "        r = requests.get(url, timeout=30)\n",
        "        r.raise_for_status()\n",
        "        with open(dst_path, 'wb') as f:\n",
        "            f.write(r.content)\n",
        "    return dst_path\n",
        "\n",
        "if USE_URLS:\n",
        "    train_path = download_if_needed(TRAIN_URL, DATA_DIR_COLAB)\n",
        "    test_path = download_if_needed(TEST_URL, DATA_DIR_COLAB)\n",
        "else:\n",
        "    train_path = os.path.join(DATA_DIR_LOCAL, 'train.csv')\n",
        "    test_path = os.path.join(DATA_DIR_LOCAL, 'test.csv')\n",
        "\n",
        "train_df = pd.read_csv(train_path)\n",
        "test_df = pd.read_csv(test_path)\n",
        "train_df.shape, test_df.shape\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Problem framing\n",
        "We are solving a binary classification problem: predict `Survived` (0/1) from passenger features.\n",
        "Common features include `Pclass`, `Sex`, `Age`, `SibSp`, `Parch`, `Fare`, and `Embarked`. The `Name`, `Ticket`, and `Cabin` fields are often noisy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "target_col = 'Survived'\n",
        "X = train_df.drop(columns=[target_col])\n",
        "y = train_df[target_col]\n",
        "X.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Quick EDA\n",
        "Goals:\n",
        "- Inspect schema, missingness, and distributions.\n",
        "- Identify candidate features for modeling and preprocessing needs.\n",
        "\n",
        "TODO: Add one more insightful plot of your choice.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "display(train_df.head())\n",
        "train_df.info()\n",
        "train_df.isna().mean().sort_values(ascending=False).to_frame('missing_frac')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(10,4))\n",
        "sns.countplot(data=train_df, x='Survived', ax=axes[0])\n",
        "axes[0].set_title('Target balance')\n",
        "sns.histplot(data=train_df, x='Age', kde=True, ax=axes[1])\n",
        "axes[1].set_title('Age distribution')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train/validation split\n",
        "We hold out a validation set to estimate generalization. We stratify by the target to preserve class balance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train, X_valid, y_train, y_valid = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "X_train.shape, X_valid.shape\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Preprocessing with ColumnTransformer\n",
        "Concepts:\n",
        "- Numeric: impute missing values (median), scale features.\n",
        "- Categorical: impute missing (most_frequent), one-hot encode.\n",
        "- `ColumnTransformer` applies different pipelines to column subsets.\n",
        "\n",
        "TODO: Choose imputation strategy for `Age` and `Embarked` (keep defaults or change).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "numeric_features = ['Age', 'Fare', 'SibSp', 'Parch']\n",
        "categorical_features = ['Pclass', 'Sex', 'Embarked']\n",
        "\n",
        "numeric_preprocess = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "categorical_preprocess = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_preprocess, numeric_features),\n",
        "        ('cat', categorical_preprocess, categorical_features),\n",
        "    ]\n",
        ")\n",
        "preprocessor\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Baseline model: Logistic Regression inside a Pipeline\n",
        "Why pipeline:\n",
        "- Guarantees preprocessing is applied consistently in training and inference.\n",
        "- Simplifies cross-validation and exporting a single object.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "clf = Pipeline(steps=[\n",
        "    ('preprocess', preprocessor),\n",
        "    ('model', LogisticRegression(max_iter=1000))\n",
        "])\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_valid)\n",
        "y_proba = clf.predict_proba(X_valid)[:, 1]\n",
        "\n",
        "acc = accuracy_score(y_valid, y_pred)\n",
        "prec = precision_score(y_valid, y_pred)\n",
        "rec = recall_score(y_valid, y_pred)\n",
        "f1 = f1_score(y_valid, y_pred)\n",
        "roc = roc_auc_score(y_valid, y_proba)\n",
        "acc, prec, rec, f1, roc\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1, 2, figsize=(10,4))\n",
        "ConfusionMatrixDisplay.from_predictions(y_valid, y_pred, ax=ax[0])\n",
        "ax[0].set_title('Confusion Matrix')\n",
        "RocCurveDisplay.from_predictions(y_valid, y_proba, ax=ax[1])\n",
        "ax[1].set_title('ROC Curve')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Hyperparameter search (small grid)\n",
        "We explore a small grid for regularization strength.\n",
        "\n",
        "TODO: Extend the grid or try a different model like `RandomForestClassifier`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "param_grid = {\n",
        "    'model__C': [0.1, 1.0, 3.0],\n",
        "    'model__penalty': ['l2'],\n",
        "    'model__solver': ['lbfgs']\n",
        "}\n",
        "grid = GridSearchCV(clf, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "grid.fit(X_train, y_train)\n",
        "grid.best_params_, grid.best_score_\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_model = grid.best_estimator_\n",
        "y_pred_best = best_model.predict(X_valid)\n",
        "y_proba_best = best_model.predict_proba(X_valid)[:, 1]\n",
        "acc_b = accuracy_score(y_valid, y_pred_best)\n",
        "prec_b = precision_score(y_valid, y_pred_best)\n",
        "rec_b = recall_score(y_valid, y_pred_best)\n",
        "f1_b = f1_score(y_valid, y_pred_best)\n",
        "roc_b = roc_auc_score(y_valid, y_proba_best)\n",
        "acc_b, prec_b, rec_b, f1_b, roc_b\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Error analysis\n",
        "Inspect where the model fails to guide improvements.\n",
        "\n",
        "TODO: Propose one hypothesis to improve, and test it (e.g., different imputation).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "errors = X_valid.copy()\n",
        "errors['y_true'] = y_valid.values\n",
        "errors['y_pred'] = y_pred_best\n",
        "errors[errors['y_true'] != errors['y_pred']].head(10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Export the best pipeline\n",
        "Exporting the entire pipeline ensures preprocessing is part of the saved artifact.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "joblib.dump(best_model, 'titanic_best_pipeline.joblib')\n",
        "os.path.getsize('titanic_best_pipeline.joblib')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Inference on test set and Kaggle-style submission\n",
        "We'll fit on the full training data using the best estimator, predict on the `test.csv`, and create `submission.csv` with columns `PassengerId` and `Survived`.\n",
        "You can upload this to Kaggle Titanic to see how it ranks relative to the benchmark.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_model.fit(X, y)\n",
        "test_pred = best_model.predict(test_df)\n",
        "submission = pd.DataFrame({\n",
        "    'PassengerId': test_df['PassengerId'],\n",
        "    'Survived': test_pred\n",
        "})\n",
        "submission.to_csv('submission.csv', index=False)\n",
        "submission.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Mini-challenge: Feature engineering to improve accuracy\n",
        "Ideas to try (implement one or more and re-evaluate):\n",
        "- `FamilySize = SibSp + Parch + 1`\n",
        "- Extract `Title` from `Name` and group rare titles\n",
        "- Bin `Age` or `Fare`\n",
        "- Try `RandomForestClassifier` or `XGBClassifier` (requires extra install)\n",
        "\n",
        "TODO: Implement `FamilySize` and include it in `numeric_features`, retrain, and compare metrics and submission score.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
